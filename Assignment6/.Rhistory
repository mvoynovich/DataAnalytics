# optimal number of clusters is 5 (based on the elbow and the silhouette)
mod.pam <- pam(abalone.data[,c(2, 3, 4, 9)], 5)
sumdiss <- mod.pam$objective[1]
sumdiss
## get and plot clustering output
assigned.clusters <- as.factor(mod.pam$cluster)
ggplot(abalone.data, aes(x = rings, y = length, colour = assigned.clusters)) +
geom_point()
## Silhouette Plot
sil <- silhouette(mod.pam$cluster, dist(abalone.data[c(2, 3, 4, 9)]))
fviz_silhouette(sil)
# Get the accuracy
knn.train.predicted <- predict(mod.knn,train[,-10])
knn.test.predicted <- predict(mod.knn,test[,-10])
knn.train.predicted <- predict(mod.knn.meas,train[,-10])
knn.test.predicted <- predict(mod.knn.meas,test[,-10])
train.cm = as.matrix(table(Actual = knn.train.true, Predicted = knn.train.predicted))
train.cm
train.accuracy <- sum(diag(train.cm))/nrow(train)
train.accuracy
test.cm = as.matrix(table(Actual = knn.test.true, Predicted = knn.test.predicted))
test.cm
test.accuracy <- sum(diag(test.cm))/nrow(test)
test.accuracy
# Best k for model
mod.knn$bestTune
# Get the accuracy
knn.train.predicted <- predict(mod.knn,train[,-10])
knn.test.predicted <- predict(mod.knn,test[,-10])
knn.train.predicted <- predict(mod.knn.meas,train[,-10])
knn.test.predicted <- predict(mod.knn.meas,test[,-10])
train.cm = as.matrix(table(Actual = knn.train.true, Predicted = knn.train.predicted))
train.cm
train.accuracy <- sum(diag(train.cm))/nrow(train)
train.accuracy
test.cm = as.matrix(table(Actual = knn.test.true, Predicted = knn.test.predicted))
test.cm
test.accuracy
# Pick best k for the KNN trained off of measurements
# Odd so there are no tie breakers
grid <- expand.grid(k = c(3, 5, 7, 9, 11))
# Train with the list of k values and pick the best based on the accuracy
# Use preprocess so all values weighted equally and scaled
mod.knn <- train(
age.group ~ length + diameter + height + rings,
data = train,
method = "knn",
preProcess = c("center", "scale"),
tuneGrid = grid,
metric="Accuracy",
trControl=ctrl
)
# Best k for model
mod.knn$bestTune
# Get the accuracy
knn.train.predicted <- predict(mod.knn,train[,-10])
knn.test.predicted <- predict(mod.knn,test[,-10])
knn.train.predicted <- predict(mod.knn.meas,train[,-10])
knn.test.predicted <- predict(mod.knn.meas,test[,-10])
train.cm = as.matrix(table(Actual = knn.train.true, Predicted = knn.train.predicted))
train.cm
train.accuracy <- sum(diag(train.cm))/nrow(train)
train.accuracy
test.cm = as.matrix(table(Actual = knn.test.true, Predicted = knn.test.predicted))
test.cm
test.accuracy <- sum(diag(test.cm))/nrow(test)
test.accuracy
source("C:/Users/matec/DataAnalytics/Lab3/Abalone_dataset_prep.R", echo = TRUE)
library(caret)
library(GGally)
library(ggplot2)
library(psych)
library(cluster)
library(dendextend)
library(colorspace)
library(factoextra)
# read dataset
abalone.data <- read.csv("C:/Users/matec/DataAnalytics/Lab3/abalone_dataset.csv")
## add new column age.group with 3 values based on the number of rings
abalone.data$age.group <- cut(abalone.data$rings, br=c(0,8,11,35), labels = c("young", 'adult', 'old'))
## alternative way of setting age.group
abalone.data$age.group[abalone.data$rings<=8] <- "young"
abalone.data$age.group[abalone.data$rings>8 & abalone.data$rings<=11] <- "adult"
abalone.data$age.group[abalone.data$rings>11 & abalone.data$rings<=35] <- "old"
## split train/test
split.rat <- 0.7
train.indexes <- sample(150,split.rat*150)
train <- abalone.data[train.indexes,]
test <- abalone.data[-train.indexes,]
# Set up the control for 10 fold cross validation to prevent overfitting when picking best k
ctrl <- trainControl(method = "cv", number = 10)
# KNN trained off of weights
# Use preprocess so all values weighted equally and scaled
mod.knn.weights <- train(age.group~whole_weight + shucked_wieght + viscera_wieght + shell_weight, data=train,
method="knn",  preProcess = c("center", "scale"), metric="Accuracy", trControl=ctrl)
knn.train.true <- train[,10]
knn.test.true <- test[,10]
knn.train.predicted <- predict(mod.knn.weights,train[,-10])
knn.test.predicted <- predict(mod.knn.weights,test[,-10])
# Get accuracy for the weight based classifier
train.cm = as.matrix(table(Actual = knn.train.true, Predicted = knn.train.predicted))
train.cm
train.accuracy <- sum(diag(train.cm))/nrow(train)
train.accuracy
test.cm = as.matrix(table(Actual = knn.test.true, Predicted = knn.test.predicted))
test.cm
test.accuracy <- sum(diag(test.cm))/nrow(test)
test.accuracy
# KNN trained off of measurements
# Use preprocess so all values weighted equally and scaled
mod.knn.meas <- train(age.group~length + diameter + height + rings, data=train, method="knn",
preProcess = c("center", "scale"), metric="Accuracy", trControl=ctrl)
knn.train.true <- train[,10]
knn.test.true <- test[,10]
knn.train.predicted <- predict(mod.knn.meas,train[,-10])
knn.test.predicted <- predict(mod.knn.meas,test[,-10])
knn.train.predicted <- predict(mod.knn.meas,train[,-10])
knn.test.predicted <- predict(mod.knn.meas,test[,-10])
# Get accuracy for measurement based classifier
train.cm = as.matrix(table(Actual = knn.train.true, Predicted = knn.train.predicted))
train.cm
train.accuracy <- sum(diag(train.cm))/nrow(train)
train.accuracy
test.cm = as.matrix(table(Actual = knn.test.true, Predicted = knn.test.predicted))
test.cm
test.accuracy <- sum(diag(test.cm))/nrow(test)
test.accuracy
# Pick best k for the KNN trained off of measurements
# Odd so there are no tie breakers
grid <- expand.grid(k = c(3, 5, 7, 9, 11))
# Train with the list of k values and pick the best based on the accuracy
# Use preprocess so all values weighted equally and scaled
mod.knn <- train(
age.group ~ length + diameter + height + rings,
data = train,
method = "knn",
preProcess = c("center", "scale"),
tuneGrid = grid,
metric="Accuracy",
trControl=ctrl
)
# Best k for model
mod.knn$bestTune
# Get the accuracy
knn.train.predicted <- predict(mod.knn,train[,-10])
knn.test.predicted <- predict(mod.knn,test[,-10])
knn.train.predicted <- predict(mod.knn.meas,train[,-10])
knn.test.predicted <- predict(mod.knn.meas,test[,-10])
train.cm = as.matrix(table(Actual = knn.train.true, Predicted = knn.train.predicted))
train.cm
train.accuracy <- sum(diag(train.cm))/nrow(train)
train.accuracy
test.cm = as.matrix(table(Actual = knn.test.true, Predicted = knn.test.predicted))
test.cm
test.accuracy <- sum(diag(test.cm))/nrow(test)
test.accuracy
library(caret)
library(GGally)
library(ggplot2)
library(psych)
library(cluster)
library(dendextend)
library(colorspace)
library(factoextra)
library(dplyr)
library(tidyverse)
library(randomForest)
setwd("C:/Users/matec/DataAnalytics/Assignment6")
df <- read.csv("C:/Users/matec/DataAnalytics/Assignment6/bank-additional-full.csv", sep=";")
df <- df %>%
mutate(across(where(is.character), as.factor))
str(df)
summary(df)
# Code/reference for data transformation for the EDA plots
# tidyr::pivot_longer(): https://tidyr.tidyverse.org/reference/pivot_longer.html
num_vars <- df %>% select(where(is.numeric))
num_vars_long <- num_vars %>%
pivot_longer(cols = everything(),
names_to = "variable",
values_to = "value")
# Plotting distributions with ggplot2 and faceting
# facet_wrap docs: https://ggplot2.tidyverse.org/reference/facet_wrap.html
ggplot(num_vars_long, aes(value)) +
geom_histogram(bins = 30, color="black") +
facet_wrap(~ variable, scales = "free")
cat_vars <- df %>% select(where(is.factor))
cat_vars_long <- cat_vars %>%
pivot_longer(cols = everything(),
names_to = "variable",
values_to = "value")
# Plotting distributions with ggplot2 and faceting
# facet_wrap docs: https://ggplot2.tidyverse.org/reference/facet_wrap.html
ggplot(cat_vars_long, aes(value)) +
geom_bar() +
facet_wrap(~ variable, scales = "free")
ggplot(df, aes(y)) + geom_bar(fill = "steelblue")
# Convert 'y' to factor just in case
df$y <- as.factor(df$y)
# -----------------------------
# Split data into train/test
# -----------------------------
train_index <- createDataPartition(df$y, p = 0.7, list = FALSE)
train <- df[train_index, ]
test <- df[-train_index, ]
# 1. Random Forest Classification for y
rf_class <- randomForest(y ~ ., data = train, importance = TRUE, ntree = 500)
# Predictions
pred_train <- predict(rf_class, train)
pred_test <- predict(rf_class, test)
# Evaluate Training Set
cm_train <- confusionMatrix(pred_train, train$y)
accuracy_train <- cm_train$overall['Accuracy']
precision_train <- cm_train$byClass['Pos Pred Value']  # precision for positive class
recall_train <- cm_train$byClass['Sensitivity']       # recall for positive class
f1_train <- 2 * ((precision_train * recall_train) / (precision_train + recall_train))
cat("Classification Performance - Training Set\n")
print(cm_train$table)  # print confusion matrix
cat("Accuracy:", accuracy_train, "\n")
cat("Precision:", precision_train, "\n")
cat("Recall:", recall_train, "\n")
cat("F1 Score:", f1_train, "\n\n")
# Evaluate Test Set
cm_test <- confusionMatrix(pred_test, test$y)
accuracy_test <- cm_test$overall['Accuracy']
precision_test <- cm_test$byClass['Pos Pred Value']
recall_test <- cm_test$byClass['Sensitivity']
f1_test <- 2 * ((precision_test * recall_test) / (precision_test + recall_test))
cat("Classification Performance - Test Set\n")
print(cm_test$table)  # print confusion matrix
cat("Accuracy:", accuracy_test, "\n")
cat("Precision:", precision_test, "\n")
cat("Recall:", recall_test, "\n")
cat("F1 Score:", f1_test, "\n")
feat_imp <- importance(rf_class)
print(feat_imp)
# 2. Logistic Regression for y
logit_model <- glm(y ~ ., data = train, family = binomial)
# Predictions (probabilities)
pred_prob_train_logit <- predict(logit_model, train, type = "response")
pred_prob_test_logit  <- predict(logit_model, test, type = "response")
# Convert probabilities to class
pred_train_logit <- ifelse(pred_prob_train_logit > 0.5, "yes", "no") %>% factor(levels = levels(df$y))
pred_test_logit  <- ifelse(pred_prob_test_logit > 0.5, "yes", "no") %>% factor(levels = levels(df$y))
# Evaluate Training Set
cm_train_logit <- confusionMatrix(pred_train_logit, train$y)
accuracy_train_logit <- cm_train_logit$overall['Accuracy']
precision_train_logit <- cm_train_logit$byClass['Pos Pred Value']
recall_train_logit <- cm_train_logit$byClass['Sensitivity']
f1_train_logit <- 2 * ((precision_train_logit * recall_train_logit) / (precision_train_logit + recall_train_logit))
cat("Logistic Regression - Training Set\n")
print(cm_train_logit$table)
cat("Accuracy:", accuracy_train_logit, "\n")
cat("Precision:", precision_train_logit, "\n")
cat("Recall:", recall_train_logit, "\n")
cat("F1 Score:", f1_train_logit, "\n\n")
# Evaluate Test Set
cm_test_logit <- confusionMatrix(pred_test_logit, test$y)
accuracy_test_logit <- cm_test_logit$overall['Accuracy']
precision_test_logit <- cm_test_logit$byClass['Pos Pred Value']
recall_test_logit <- cm_test_logit$byClass['Sensitivity']
f1_test_logit <- 2 * ((precision_test_logit * recall_test_logit) / (precision_test_logit + recall_test_logit))
cat("Logistic Regression - Test Set\n")
print(cm_test_logit$table)
cat("Accuracy:", accuracy_test_logit, "\n")
cat("Precision:", precision_test_logit, "\n")
cat("Recall:", recall_test_logit, "\n")
cat("F1 Score:", f1_test_logit, "\n\n")
# 3. KNN Regression for campaign
predictors <- train %>% select(-campaign)
response <- train$campaign
# Define training control
train_control <- trainControl(method = "cv", number = 5)  # 5-fold CV
# Train kNN regression model
knn_model <- train(
x = predictors,
y = response,
method = "knn",
trControl = train_control,
preProcess = c("center", "scale"),  # normalize features
tuneLength = 5  # test 5 different k values
)
# Train linear regression model
lm_campaign <- lm(campaign ~ ., data = train)
# Predictions
pred_train <- predict(lm_campaign, newdata = train)
pred_test  <- predict(lm_campaign, newdata = test)
# Evaluate regression with Mean Squared Error
mse_train <- mean((train$campaign - pred_train)^2)
mse_test  <- mean((test$campaign - pred_test)^2)
cat("Linear Regression MSE - Training Set:", mse_train, "\n")
cat("Linear Regression MSE - Test Set:", mse_test, "\n")
mae_train <- mean(abs(train$campaign - pred_train))
mae_test  <- mean(abs(test$campaign - pred_test))
cat("Linear Regression MSE - Training Set:", mse_train, "\n")
cat("Linear Regression MSE - Test Set:", mse_test, "\n")
cat("Linear Regression Mean Absolute Error - Training Set:", mae_train, "\n")
cat("Linear Regression Mean Absolute Error - Test Set:", mae_test, "\n")
ggplot(data = data.frame(Predicted = pred_train, Residuals = residuals_train),
aes(x = Predicted, y = Residuals)) +
geom_point(color = "steelblue", alpha = 0.6) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(title = "Residual Plot - Linear Regression (campaign)",
x = "Predicted Values",
y = "Residuals") +
theme_minimal()
residuals_train <- train$campaign - pred_train
ggplot(data = data.frame(Predicted = pred_train, Residuals = residuals_train),
aes(x = Predicted, y = Residuals)) +
geom_point(color = "steelblue", alpha = 0.6) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(title = "Residual Plot - Linear Regression (campaign)",
x = "Predicted Values",
y = "Residuals") +
theme_minimal()
# Coefficients (weights)
weights <- coef(lm_campaign)
print(weights)
# -----------------------------
# Print model coefficients (weights)
# -----------------------------
weights <- coef(lm_quad)
# 3. KNN Regression for campaign
numeric_vars <- names(train %>% select(where(is.numeric), -campaign))
# Create quadratic terms
quad_terms <- paste0("I(", numeric_vars, "^2)", collapse = " + ")
# Build formula with original predictors + quadratic terms
formula_quad <- as.formula(paste("campaign ~ . +", quad_terms))
# Fit the polynomial regression
lm_quad <- lm(formula_quad, data = train)
# Predictions
pred_train <- predict(lm_quad, newdata = train)
pred_test  <- predict(lm_quad, newdata = test)
# Evaluate regression
mse_train <- mean((train$campaign - pred_train)^2)
mse_test  <- mean((test$campaign - pred_test)^2)
mae_train <- mean(abs(train$campaign - pred_train))
mae_test  <- mean(abs(test$campaign - pred_test))
cat("Polynomial Regression (Quadratic) - Training Set\n")
cat("MSE:", mse_train, " | MAE:", mae_train, "\n\n")
cat("Polynomial Regression (Quadratic) - Test Set\n")
cat("MSE:", mse_test, " | MAE:", mae_test, "\n\n")
# -----------------------------
# Residual Plot
# -----------------------------
library(ggplot2)
residuals_train <- train$campaign - pred_train
ggplot(data = data.frame(Predicted = pred_train, Residuals = residuals_train),
aes(x = Predicted, y = Residuals)) +
geom_point(color = "steelblue", alpha = 0.6) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(title = "Residual Plot - Polynomial Regression (campaign)",
x = "Predicted Values",
y = "Residuals") +
theme_minimal()
# -----------------------------
# Print model coefficients (weights)
# -----------------------------
weights <- coef(lm_quad)
# 3. KNN Regression for campaign
predictors <- train %>% select(-campaign)
response <- train$campaign
# Define training control
train_control <- trainControl(method = "cv", number = 5)  # 5-fold CV
# Train linear regression model
lm_campaign <- lm(campaign ~ ., data = train)
# Predictions
pred_train <- predict(lm_campaign, newdata = train)
pred_test  <- predict(lm_campaign, newdata = test)
# Evaluate regression with Mean Squared Error
mse_train <- mean((train$campaign - pred_train)^2)
mse_test  <- mean((test$campaign - pred_test)^2)
mae_train <- mean(abs(train$campaign - pred_train))
mae_test  <- mean(abs(test$campaign - pred_test))
cat("Linear Regression MSE - Training Set:", mse_train, "\n")
cat("Linear Regression MSE - Test Set:", mse_test, "\n")
cat("Linear Regression Mean Absolute Error - Training Set:", mae_train, "\n")
cat("Linear Regression Mean Absolute Error - Test Set:", mae_test, "\n")
residuals_train <- train$campaign - pred_train
ggplot(data = data.frame(Predicted = pred_train, Residuals = residuals_train),
aes(x = Predicted, y = Residuals)) +
geom_point(color = "steelblue", alpha = 0.6) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(title = "Residual Plot - Linear Regression (campaign)",
x = "Predicted Values",
y = "Residuals") +
theme_minimal()
# Coefficients (weights)
weights <- coef(lm_campaign)
print(weights)
# 3. Linear Regression for campaign
predictors <- train %>% select(-campaign)
response <- train$campaign
# Define training control
train_control <- trainControl(method = "cv", number = 5)  # 5-fold CV
# Train linear regression model
lm_campaign <- lm(campaign ~ ., data = train)
# Predictions
pred_train <- predict(lm_campaign, newdata = train)
pred_test  <- predict(lm_campaign, newdata = test)
# Evaluate regression with Mean Squared Error
mse_train <- mean((train$campaign - pred_train)^2)
mse_test  <- mean((test$campaign - pred_test)^2)
mae_train <- mean(abs(train$campaign - pred_train))
mae_test  <- mean(abs(test$campaign - pred_test))
cat("Linear Regression MSE - Training Set:", mse_train, "\n")
cat("Linear Regression MSE - Test Set:", mse_test, "\n")
cat("Linear Regression Mean Absolute Error - Training Set:", mae_train, "\n")
cat("Linear Regression Mean Absolute Error - Test Set:", mae_test, "\n")
residuals_train <- train$campaign - pred_train
ggplot(data = data.frame(Predicted = pred_train, Residuals = residuals_train),
aes(x = Predicted, y = Residuals)) +
geom_point(color = "steelblue", alpha = 0.6) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(title = "Residual Plot - Linear Regression (campaign)",
x = "Predicted Values",
y = "Residuals") +
theme_minimal()
# Coefficients (weights)
weights <- coef(lm_campaign)
print(weights)
summary(df)
# Code/reference for data transformation for the EDA plots
# tidyr::pivot_longer(): https://tidyr.tidyverse.org/reference/pivot_longer.html
num_vars <- df %>% select(where(is.numeric))
num_vars_long <- num_vars %>%
pivot_longer(cols = everything(),
names_to = "variable",
values_to = "value")
# Plotting distributions with ggplot2 and faceting
# facet_wrap docs: https://ggplot2.tidyverse.org/reference/facet_wrap.html
ggplot(num_vars_long, aes(value)) +
geom_histogram(bins = 30, color="black") +
facet_wrap(~ variable, scales = "free")
cat_vars <- df %>% select(where(is.factor))
cat_vars_long <- cat_vars %>%
pivot_longer(cols = everything(),
names_to = "variable",
values_to = "value")
# Plotting distributions with ggplot2 and faceting
# facet_wrap docs: https://ggplot2.tidyverse.org/reference/facet_wrap.html
ggplot(cat_vars_long, aes(value)) +
geom_bar() +
facet_wrap(~ variable, scales = "free")
ggplot(df, aes(y)) + geom_bar(fill = "steelblue")
# Convert 'y' to factor just in case
df$y <- as.factor(df$y)
# Evaluate Training Set
cm_train <- confusionMatrix(pred_train, train$y)
accuracy_train <- cm_train$overall['Accuracy']
precision_train <- cm_train$byClass['Pos Pred Value']  # precision for positive class
recall_train <- cm_train$byClass['Sensitivity']       # recall for positive class
f1_train <- 2 * ((precision_train * recall_train) / (precision_train + recall_train))
cat("Classification Performance - Training Set\n")
print(cm_train$table)  # print confusion matrix
cat("Accuracy:", accuracy_train, "\n")
cat("Precision:", precision_train, "\n")
cat("Recall:", recall_train, "\n")
cat("F1 Score:", f1_train, "\n\n")
# Evaluate Test Set
cm_test <- confusionMatrix(pred_test, test$y)
accuracy_test <- cm_test$overall['Accuracy']
precision_test <- cm_test$byClass['Pos Pred Value']
recall_test <- cm_test$byClass['Sensitivity']
f1_test <- 2 * ((precision_test * recall_test) / (precision_test + recall_test))
cat("Classification Performance - Test Set\n")
print(cm_test$table)  # print confusion matrix
cat("Accuracy:", accuracy_test, "\n")
cat("Precision:", precision_test, "\n")
cat("Recall:", recall_test, "\n")
cat("F1 Score:", f1_test, "\n")
feat_imp <- importance(rf_class)
print(feat_imp)
# Evaluate Training Set
cm_train_logit <- confusionMatrix(pred_train_logit, train$y)
accuracy_train_logit <- cm_train_logit$overall['Accuracy']
precision_train_logit <- cm_train_logit$byClass['Pos Pred Value']
recall_train_logit <- cm_train_logit$byClass['Sensitivity']
f1_train_logit <- 2 * ((precision_train_logit * recall_train_logit) / (precision_train_logit + recall_train_logit))
cat("Logistic Regression - Training Set\n")
print(cm_train_logit$table)
cat("Accuracy:", accuracy_train_logit, "\n")
cat("Precision:", precision_train_logit, "\n")
cat("Recall:", recall_train_logit, "\n")
cat("F1 Score:", f1_train_logit, "\n\n")
# Evaluate Test Set
cm_test_logit <- confusionMatrix(pred_test_logit, test$y)
accuracy_test_logit <- cm_test_logit$overall['Accuracy']
precision_test_logit <- cm_test_logit$byClass['Pos Pred Value']
recall_test_logit <- cm_test_logit$byClass['Sensitivity']
f1_test_logit <- 2 * ((precision_test_logit * recall_test_logit) / (precision_test_logit + recall_test_logit))
cat("Logistic Regression - Test Set\n")
print(cm_test_logit$table)
cat("Accuracy:", accuracy_test_logit, "\n")
cat("Precision:", precision_test_logit, "\n")
cat("Recall:", recall_test_logit, "\n")
cat("F1 Score:", f1_test_logit, "\n\n")
cat("Linear Regression MSE - Training Set:", mse_train, "\n")
cat("Linear Regression MSE - Test Set:", mse_test, "\n")
cat("Linear Regression Mean Absolute Error - Training Set:", mae_train, "\n")
cat("Linear Regression Mean Absolute Error - Test Set:", mae_test, "\n")
residuals_train <- train$campaign - pred_train
ggplot(data = data.frame(Predicted = pred_train, Residuals = residuals_train),
aes(x = Predicted, y = Residuals)) +
geom_point(color = "steelblue", alpha = 0.6) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(title = "Residual Plot - Linear Regression (campaign)",
x = "Predicted Values",
y = "Residuals") +
theme_minimal()
# Coefficients (weights)
weights <- coef(lm_campaign)
print(weights)
