p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted
accuracy <- sum(diag)/n
accuracy
recall = diag / rowsums
precision = diag / colsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
#Test svm model 2
test.pred <- predict(svm.mod2, test)
cm = as.matrix(table(Actual = test$Type, Predicted = test.pred))
cm
n = sum(cm) # number of instances
nc = nrow(cm) # number of classes
diag = diag(cm) # number of correctly classified instances per class
rowsums = apply(cm, 1, sum) # number of instances per class
colsums = apply(cm, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted
accuracy <- sum(diag)/n
accuracy
recall = diag / rowsums
precision = diag / colsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
# Test knn
test.pred <- predict(knn.mod, test)
cm = as.matrix(table(Actual = test$Type, Predicted = test.pred))
cm
n = sum(cm) # number of instances
nc = nrow(cm) # number of classes
diag = diag(cm) # number of correctly classified instances per class
rowsums = apply(cm, 1, sum) # number of instances per class
colsums = apply(cm, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted
accuracy <- sum(diag)/n
accuracy
recall = diag / rowsums
precision = diag / colsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
knn.mod$bestTune
plot(svm.mod1, train, Alcohol~Flavanoids)
## load libraries
library(ggplot2)
library(ggfortify)
library(GGally)
library(e1071)
library(class)
library(psych)
library(readr)
## set working directory so that files can be referenced without the full path
setwd("C:/Users/matec/DataAnalytics/Lab6/")
println(nice work)
## set working directory so that files can be referenced without the full path
setwd("C:/Users/matec/DataAnalytics/Lab6/")
df <- read.csv("C:/Users/matec/DataAnalytics/NY-House-Dataset.csv")
df <- read.csv("C:/Users/matec/DataAnalytics/lab6/NY-House-Dataset.csv")
## keep only variables needed
df <- df[, c("PRICE", "PROPERTYSQFT", "BEDS", "BATH")]
## remove missing values
df <- na.omit(df)
N <- nrow(df)
train.indexes <- sample(N,0.8*N)
train <- df[train.indexes,]
test <- df[-train.indexes,]
# Train a linear model
lm.mod <- lm(PRICE ~ ., data = train)
lm_pred <- predict(lm_model, newdata = test)
lm_pred <- predict(lm.mod, test)
knn.pred <- predict(knn.model, test)
knn.model <- train(
PRICE ~ .,
data = train,
method = "knn",
preProcess = c("center", "scale"),
tuneGrid = data.frame(k = 5),
trControl = ctrl
)
# Train a KNN
ctrl <- trainControl(method = "cv", number = 10)
knn.model <- train(
PRICE ~ .,
data = train,
method = "knn",
preProcess = c("center", "scale"),
tuneGrid = data.frame(k = 5),
trControl = ctrl
)
knn.pred <- predict(knn.model, test)
# Train a SVM
svm.mod <- svm(
PRICE ~ .,
data = train,
type = "eps-regression",
kernel = "radial"
)
svm.pred <- predict(svm.mod, test)
lm_mae  <- mean(abs(test$PRICE - lm_pred))
lm.pred <- predict(lm.mod, test)
lm_mae  <- mean(abs(test$PRICE - lm_pred))
lm_mse  <- mean((test$PRICE - lm_pred)^2)
lm_rmse <- sqrt(lm_mse)
knn_mae  <- mean(abs(test$PRICE - knn.pred))
knn_mse  <- mean((test$PRICE - knn.pred)^2)
knn_rmse <- sqrt(knn_mse)
svm_mae  <- mean(abs(test$PRICE - svm.pred))
svm_mse  <- mean((test$PRICE - svm.pred)^2)
svm_rmse <- sqrt(svm_mse)
# Linear model results
lm_mae  <- mean(abs(test$PRICE - lm_pred))
lm_mse  <- mean((test$PRICE - lm_pred)^2)
lm_rmse <- sqrt(lm_mse)
lm_mae
lm_mse
lm_rmse
# KNN model results
knn_mae  <- mean(abs(test$PRICE - knn.pred))
knn_mse  <- mean((test$PRICE - knn.pred)^2)
knn_rmse <- sqrt(knn_mse)
knn_mae
knn_mse
knn_rmse
# SVM model results
svm_mae  <- mean(abs(test$PRICE - svm.pred))
svm_mse  <- mean((test$PRICE - svm.pred)^2)
svm_rmse <- sqrt(svm_mse)
svm_mae
svm_mse
svm_rmse
## load libraries
library(ggplot2)
library(ggfortify)
library(GGally)
library(e1071)
library(class)
library(psych)
library(readr)
## set working directory so that files can be referenced without the full path
setwd("C:/Users/matec/DataAnalytics/Lab6/")
df <- read.csv("C:/Users/matec/DataAnalytics/lab6/NY-House-Dataset.csv")
## keep only variables needed
df <- df[, c("PRICE", "PROPERTYSQFT")]
## remove missing values
df <- na.omit(df)
N <- nrow(df)
train.indexes <- sample(N,0.8*N)
train <- df[train.indexes,]
test <- df[-train.indexes,]
# Train a linear model
lm.mod <- lm(PRICE ~ ., data = train)
lm.pred <- predict(lm.mod, test)
# Train a KNN
ctrl <- trainControl(method = "cv", number = 10)
knn.mod <- train(
PRICE ~ .,
data = train,
method = "knn",
preProcess = c("center", "scale"),
tuneGrid = data.frame(k = 5),
trControl = ctrl
)
## load libraries
library(ggplot2)
library(ggfortify)
library(GGally)
library(e1071)
library(class)
library(psych)
library(readr)
## set working directory so that files can be referenced without the full path
setwd("C:/Users/matec/DataAnalytics/Lab6/")
df <- read.csv("C:/Users/matec/DataAnalytics/lab6/NY-House-Dataset.csv")
## keep only variables needed
df <- df[, c("PRICE", "PROPERTYSQFT")]
## remove missing values
df <- na.omit(df)
N <- nrow(df)
train.indexes <- sample(N,0.8*N)
train <- df[train.indexes,]
test <- df[-train.indexes,]
# Train a linear model
lm.mod <- lm(PRICE ~ ., data = train)
lm.pred <- predict(lm.mod, test)
# Train a KNN
ctrl <- trainControl(method = "cv", number = 10)
knn.mod <- train(
PRICE ~ .,
data = train,
method = "knn",
preProcess = c("center", "scale"),
tuneGrid = data.frame(k = 5),
trControl = ctrl
)
knn.mod <- train(
PRICE ~ PROPERTYSQFT,
data = train,
method = "knn",
preProcess = c("center", "scale"),
tuneGrid = data.frame(k = 5),
trControl = ctrl
)
str(train$PROPERTYSQFT)
# Train a KNN
knn.mod <- train(
PRICE ~ PROPERTYSQFT,
data = train,
method = "knn",
preProcess = c("center", "scale"),
tuneGrid = data.frame(k = 5)
)
# Train a RF
ctrl <- trainControl(method="cv", number=5)
rf.mod <- train(
PRICE ~ .,
data = train,
method = "rf",
trControl = ctrl,
importance = TRUE
)
## load libraries
library(ggplot2)
library(ggfortify)
library(GGally)
library(e1071)
library(class)
library(psych)
library(readr)
library(randomForest)
## set working directory so that files can be referenced without the full path
setwd("C:/Users/matec/DataAnalytics/Lab6/")
df <- read.csv("C:/Users/matec/DataAnalytics/lab6/NY-House-Dataset.csv")
## keep only variables needed
df <- df[, c("PRICE", "PROPERTYSQFT")]
## remove missing values
df <- na.omit(df)
N <- nrow(df)
train.indexes <- sample(N,0.8*N)
train <- df[train.indexes,]
test <- df[-train.indexes,]
# Train a linear model
lm.mod <- lm(PRICE ~ ., data = train)
lm.pred <- predict(lm.mod, test)
# Train a RF
rf.mod <- train(
PRICE ~ .,
data = train,
method = "rf",
importance = TRUE
)
# Train a Decision Tree
ctrl <- trainControl(method="cv", number=5)  # 5-fold cross-validation
tree.mod <- train(
PRICE ~ .,
data = train,
method = "rpart",
trControl = ctrl,
tuneLength = 10
)
tree.pred <- predict(tree.mod, test)
# Train a SVM
svm.mod <- svm(
PRICE ~ .,
data = train,
type = "eps-regression",
kernel = "radial"
)
svm.pred <- predict(svm.mod, test)
## load libraries
library(ggplot2)
library(ggfortify)
library(GGally)
library(e1071)
library(class)
library(psych)
library(readr)
library(randomForest)
library(rpart)
## set working directory so that files can be referenced without the full path
setwd("C:/Users/matec/DataAnalytics/Lab6/")
df <- read.csv("C:/Users/matec/DataAnalytics/lab6/NY-House-Dataset.csv")
## keep only variables needed
df <- df[, c("PRICE", "PROPERTYSQFT")]
## remove missing values
df <- na.omit(df)
N <- nrow(df)
train.indexes <- sample(N,0.8*N)
train <- df[train.indexes,]
test <- df[-train.indexes,]
# Train a linear model
lm.mod <- lm(PRICE ~ ., data = train)
lm.pred <- predict(lm.mod, test)
# Train a Decision Tree
ctrl <- trainControl(method="cv", number=5)  # 5-fold cross-validation
tree.mod <- train(
PRICE ~ .,
data = train,
method = "rpart",
trControl = ctrl,
tuneLength = 10
)
tree.pred <- predict(tree.mod, test)
# Train a SVM
svm.mod <- svm(
PRICE ~ .,
data = train,
type = "eps-regression",
kernel = "radial"
)
svm.pred <- predict(svm.mod, test)
# Linear model results
lm_mae  <- mean(abs(test$PRICE - lm_pred))
## load libraries
library(ggplot2)
library(ggfortify)
library(GGally)
library(e1071)
library(class)
library(psych)
library(readr)
library(randomForest)
library(rpart)
## set working directory so that files can be referenced without the full path
setwd("C:/Users/matec/DataAnalytics/Lab6/")
df <- read.csv("C:/Users/matec/DataAnalytics/lab6/NY-House-Dataset.csv")
## keep only variables needed
df <- df[, c("PRICE", "PROPERTYSQFT")]
## remove missing values
df <- na.omit(df)
N <- nrow(df)
train.indexes <- sample(N,0.8*N)
train <- df[train.indexes,]
test <- df[-train.indexes,]
# Train a linear model
lm.mod <- lm(PRICE ~ ., data = train)
lm.pred <- predict(lm.mod, test)
# Train a Decision Tree
ctrl <- trainControl(method="cv", number=5)  # 5-fold cross-validation
tree.mod <- train(
PRICE ~ .,
data = train,
method = "rpart",
trControl = ctrl,
tuneLength = 10
)
tree.pred <- predict(tree.mod, test)
# Train a SVM
svm.mod <- svm(
PRICE ~ .,
data = train,
type = "eps-regression",
kernel = "radial"
)
svm.pred <- predict(svm.mod, test)
# Linear model results
lm_mae  <- mean(abs(test$PRICE - lm.pred))
lm_mse  <- mean((test$PRICE - lm.pred)^2)
lm_rmse <- sqrt(lm_mse)
lm_mae
lm_mse
lm_rmse
# tree model results
tree_mae  <- mean(abs(test$PRICE - tree.pred))
tree_mse  <- mean((test$PRICE - tree.pred)^2)
tree_rmse <- sqrt(tree_mse)
tree_mae
tree_mse
tree_rmse
# SVM model results
svm_mae  <- mean(abs(test$PRICE - svm.pred))
svm_mse  <- mean((test$PRICE - svm.pred)^2)
svm_rmse <- sqrt(svm_mse)
svm_mae
svm_mse
svm_rmse
## load libraries
library(ggplot2)
library(ggfortify)
library(GGally)
library(e1071)
library(class)
library(psych)
library(readr)
library(randomForest)
library(rpart)
## set working directory so that files can be referenced without the full path
setwd("C:/Users/matec/DataAnalytics/Lab6/")
df <- read.csv("C:/Users/matec/DataAnalytics/lab6/NY-House-Dataset.csv")
## keep only variables needed
df <- df[, c("PRICE", "PROPERTYSQFT")]
## remove missing values
df <- na.omit(df)
## remove outliers based on IQR
Q1 <- quantile(df$PRICE, 0.25)
Q3 <- quantile(df$PRICE, 0.75)
IQR_val <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR_val
upper_bound <- Q3 + 1.5 * IQR_val
df <- df[df$PRICE >= lower_bound & df$PRICE <= upper_bound, ]
# test train split
N <- nrow(df)
train.indexes <- sample(N,0.8*N)
train <- df[train.indexes,]
test <- df[-train.indexes,]
# Train a linear model
lm.mod <- lm(PRICE ~ ., data = train)
lm.pred <- predict(lm.mod, test)
# Train a Decision Tree
ctrl <- trainControl(method="cv", number=5)  # 5-fold cross-validation
tree.mod <- train(
PRICE ~ .,
data = train,
method = "rpart",
trControl = ctrl,
tuneLength = 10
)
tree.pred <- predict(tree.mod, test)
# Train a SVM
svm.mod <- svm(
PRICE ~ .,
data = train,
type = "eps-regression",
kernel = "radial"
)
svm.pred <- predict(svm.mod, test)
# Linear model results
lm_mae  <- mean(abs(test$PRICE - lm.pred))
lm_mse  <- mean((test$PRICE - lm.pred)^2)
lm_rmse <- sqrt(lm_mse)
lm_mae
lm_mse
lm_rmse
# tree model results
tree_mae  <- mean(abs(test$PRICE - tree.pred))
tree_mse  <- mean((test$PRICE - tree.pred)^2)
tree_rmse <- sqrt(tree_mse)
tree_mae
tree_mse
tree_rmse
# SVM model results
svm_mae  <- mean(abs(test$PRICE - svm.pred))
svm_mse  <- mean((test$PRICE - svm.pred)^2)
svm_rmse <- sqrt(svm_mse)
svm_mae
svm_mse
svm_rmse
library(caret)
library(GGally)
library(ggplot2)
library(psych)
library(cluster)
library(dendextend)
library(colorspace)
library(factoextra)
library(dplyr)
library(tidyverse)
library(randomForest)
setwd("C:/Users/matec/DataAnalytics/Project")
df <- read.csv("C:/Users/matec/DataAnalytics/europe_tourism_gdp.csv")
df <- df %>%
mutate(across(where(is.character), as.factor))
str(df)
summary(df)
# Code/reference for data transformation for the EDA plots
# tidyr::pivot_longer(): https://tidyr.tidyverse.org/reference/pivot_longer.html
num_vars <- df %>% select(where(is.numeric))
num_vars_long <- num_vars %>%
pivot_longer(cols = everything(),
names_to = "variable",
values_to = "value")
# Plotting distributions with ggplot2 and faceting
# facet_wrap docs: https://ggplot2.tidyverse.org/reference/facet_wrap.html
ggplot(num_vars_long, aes(value)) +
geom_histogram(bins = 30, color="gray") +
facet_wrap(~ variable, scales = "free")
cat_vars <- df %>% select(where(is.factor))
cat_vars_long <- cat_vars %>%
pivot_longer(cols = everything(),
names_to = "variable",
values_to = "value")
# Plotting distributions with ggplot2 and faceting
# facet_wrap docs: https://ggplot2.tidyverse.org/reference/facet_wrap.html
ggplot(cat_vars_long, aes(value)) +
geom_bar() +
facet_wrap(~ variable, scales = "free")
ggplot(df, aes(GDP_constant_USD)) + geom_bar(fill = "steelblue")
# GDP vs Tourism Receipts
ggplot(df, aes(x = Tourism_receipts_USD,
y = GDP_constant_USD)) +
geom_point(alpha = 0.6, color = "steelblue") +
geom_smooth(method = "lm", se = FALSE, color = "darkred") +
scale_x_continuous(labels = scales::comma) +
scale_y_continuous(labels = scales::comma) +
labs(title = "GDP vs Tourism Receipts",
x = "Tourism Receipts (USD)",
y = "GDP (constant USD)") +
theme_minimal()
# GDP vs Tourism Arrivals
ggplot(df, aes(x = Tourism_arrivals,
y = GDP_constant_USD)) +
geom_point(alpha = 0.6, color = "purple") +
geom_smooth(method = "lm", se = FALSE, color = "black") +
scale_y_continuous(labels = scales::comma) +
labs(title = "GDP vs Tourism Arrivals",
x = "Tourism Arrivals",
y = "GDP (constant USD)") +
theme_minimal()
ggpairs(df %>%
select(GDP_constant_USD,
Tourism_receipts_USD,
Tourism_arrivals))
